{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will try to solve the classic problem of matrix factorization using the gradient descent algorithm. In particular, I will factorize a matrix $D$ such that $D \\approx CF$. In order to do so, I will use gradient descent in order to train the matrices entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ D =\\begin{bmatrix} \n",
    "        d_{11} & d_{12} & \\dots & d_{1m} \\\\\n",
    "        d_{21} &  &  \\\\\n",
    "        \\vdots &  & \\ddots \\\\\n",
    "        d_{n1} &  & & d_{nm} \\\\\n",
    "        \\end{bmatrix}    C =\\begin{bmatrix} \n",
    "        c_{11} & c_{12} & \\dots & c_{1k} \\\\\n",
    "        c_{21} &  &  \\\\\n",
    "        \\vdots &  & \\ddots \\\\\n",
    "        c_{n1} &  & & c_{nk} \\\\\n",
    "        \\end{bmatrix}  F =\\begin{bmatrix} \n",
    "        f_{11} & f_{12} & \\dots & f_{1m} \\\\\n",
    "        f_{21} &  &  \\\\\n",
    "        \\vdots &  & \\ddots \\\\\n",
    "        f_{k1} &  & & f_{km} \\\\\n",
    "        \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we let $F^{i}$ denote the ith column of F, then we have $d_{i,j} \\approx C^{Ti} \\cdot F^{i}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import torch                                         # Our main library         \n",
    "from torch import autograd                           # Needed to compute gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let us generate the D matrix. And initializing the C and F matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0456,  0.1537,  0.0830,  0.0980],\n",
      "        [ 0.2683,  0.4260,  0.4277,  0.4913],\n",
      "        [ 0.4779,  0.7156,  0.7563,  0.8673]])\n"
     ]
    }
   ],
   "source": [
    "# Define Dimensions\n",
    "n = 3\n",
    "m = 4\n",
    "k = 2\n",
    "\n",
    "D = torch.mm(torch.rand(n,k),torch.rand(k,m))\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9721,  0.2518],\n",
      "        [ 0.0386,  0.5858],\n",
      "        [ 0.4206,  0.2610]]) \n",
      " tensor([[ 0.8656,  0.0336,  0.1317,  0.2444],\n",
      "        [ 0.4919,  0.4990,  0.4563,  0.7689]])\n"
     ]
    }
   ],
   "source": [
    "# Since C and F are going to be trained, we set requires_grad=True in order for pytorch to track the operations we do with them\n",
    "\n",
    "C = torch.rand(n,k, requires_grad=True)\n",
    "F = torch.rand(k,m, requires_grad=True)\n",
    "print(C, '\\n', F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the weights of C and F, we need to compute the new matrix D_hat as a product between the two matrices. The newly computed matrix will keep track of the gradient w.r.t to the weights of C and F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9654,  0.1583,  0.2429,  0.4312],\n",
      "        [ 0.3215,  0.2936,  0.2724,  0.4599],\n",
      "        [ 0.4925,  0.1443,  0.1745,  0.3034]])\n"
     ]
    }
   ],
   "source": [
    "D_hat = torch.mm(C,F)\n",
    "print(D_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MmBackward object at 0x000001E7B1B99E80>\n"
     ]
    }
   ],
   "source": [
    "print(D_hat.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we compute D_hat, we need to compute our loss function. We will sum the squared differences of the components of D_hat and D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(A, B):\n",
    "    \"\"\"\n",
    "    Computes the component by component sum of the squared differences\n",
    "    \"\"\"\n",
    "    return (torch.sum((A-B)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0110)\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(D, D_hat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SumBackward0 object at 0x000001E7B1B99860>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to do some backpropagation. The brackpropagation algorothm helps us efficiently compute the gradient, going 'backward'. Let us denote the loss function as $\\ell$. Remember thst the loss is a function of C and F. So we have $\\ell(C,F)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the numerical gradients computed for each input matrix. For instance, $GC_{i,j}(C,F) = \\frac{\\partial \\ell}{\\partial C_{i,j}}(C,F) $. Note that the gradient and the loss function are computed for some specific values of C and F. If we change C and F, the loss and its gradient are also going to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7976,  1.5676],\n",
      "        [ 0.0270, -0.2698],\n",
      "        [-0.4419, -1.9538]])\n"
     ]
    }
   ],
   "source": [
    "print(C.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.8045, -0.4818, -0.1906,  0.1710],\n",
      "        [ 0.5332, -0.4509, -0.4052, -0.1633]])\n"
     ]
    }
   ],
   "source": [
    "print(F.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we computed the gradient, we can set a learning rate parameted and update the weights. In order to udate the weights, we will simply update them in the direction of greatest descent of the loss function, which is negative the gradient. The learning rate is a parameter that tells us how fast we will update the weights.\n",
    "Remember to zero out the gradients once this step is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.01\n",
    "\n",
    "# We need to use NO_GRAD to keep the update out of the gradient computation\n",
    "# Why is that? It boils down to the DYNAMIC GRAPH that PyTorch uses...\n",
    "with torch.no_grad():\n",
    "    C -=  lr * C.grad\n",
    "    F -=  lr * F.grad\n",
    "    \n",
    "C.grad.zero_()\n",
    "F.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9541,  0.2361],\n",
      "        [ 0.0383,  0.5885],\n",
      "        [ 0.4250,  0.2805]]) \n",
      " tensor([[ 0.8476,  0.0384,  0.1336,  0.2426],\n",
      "        [ 0.4866,  0.5036,  0.4603,  0.7705]])\n"
     ]
    }
   ],
   "source": [
    "print(C, '\\n', F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the new C and F matrices, we can see that they have been updated. Also, in the net chinck we are going to check that the loss has indeed decreased with this new weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8741)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    D_hat = torch.mm(C,F)\n",
    "    print(criterion(D, D_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has indeed decreased. Let us now build a loop and train the matrices for 100 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration n.  1 . Loss:  1.8741\n",
      "Iteration n.  1000 . Loss:  0.0223\n",
      "Iteration n.  2000 . Loss:  0.0060\n",
      "Iteration n.  3000 . Loss:  0.0028\n",
      "Iteration n.  4000 . Loss:  0.0020\n",
      "Iteration n.  5000 . Loss:  0.0017\n",
      "Iteration n.  6000 . Loss:  0.0016\n",
      "Iteration n.  7000 . Loss:  0.0015\n",
      "Iteration n.  8000 . Loss:  0.0015\n",
      "Iteration n.  9000 . Loss:  0.0014\n",
      "Iteration n.  10000 . Loss:  0.0014\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 10000\n",
    "lr = 0.001\n",
    "loss_history = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    \n",
    "    # Step 0: 0 out gradients\n",
    "    C.grad.zero_()\n",
    "    F.grad.zero_()\n",
    "    # 1st step: forward pass\n",
    "    D_hat = torch.mm(C,F)\n",
    "    loss = criterion(D, D_hat)\n",
    "    # 2nd step: backprop\n",
    "    loss.backward()\n",
    "    # 3rd step: parmeters updating\n",
    "    with torch.no_grad():\n",
    "        C -=  lr * C.grad\n",
    "        F -=  lr * F.grad\n",
    "        loss_history.append(float(loss.detach().numpy()))\n",
    "        if (i+1)%1000 == 0 or i == 0:\n",
    "            print('Iteration n. ', '{:d}'.format(i+1), '. Loss: ', '{:.4f}'.format(loss_history[i]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the training is complete let's plot D_hat and D, plus their difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0497,  0.1423,  0.0869,  0.1019],\n",
      "        [ 0.2768,  0.4008,  0.4364,  0.5000],\n",
      "        [ 0.4724,  0.7316,  0.7508,  0.8618]]) \n",
      " tensor([[ 0.0456,  0.1537,  0.0830,  0.0980],\n",
      "        [ 0.2683,  0.4260,  0.4277,  0.4913],\n",
      "        [ 0.4779,  0.7156,  0.7563,  0.8673]])\n"
     ]
    }
   ],
   "source": [
    "print(D_hat, '\\n',D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-02 *\n",
      "       [[ 0.4044, -1.1402,  0.3885,  0.3853],\n",
      "        [ 0.8437, -2.5201,  0.8680,  0.8727],\n",
      "        [-0.5418,  1.6038, -0.5506, -0.5531]])\n"
     ]
    }
   ],
   "source": [
    "print(D_hat-D)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
