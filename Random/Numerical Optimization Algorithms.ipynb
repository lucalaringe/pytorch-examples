{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Optimization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will try to implement some numerical optimization algorithms and assess their performances. The examples follow Chapter 5 of Hamilton's time-series analysis textbook. The function to maximize will be $\\ell(\\phi)$, which in this particular case is the log-likelihood of an AR(1) process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autograd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rc('figure', figsize=(14, 8))\n",
    "plt.rc('grid', color='gray', alpha = 0.3, linestyle='solid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to maximize (log-likelihood of an AR(1) process)\n",
    "\n",
    "def log_likelihood_float(phi):\n",
    "    y = torch.Tensor([0.8,0.2,-1.2,-0.4,0])\n",
    "    T = 5\n",
    "    s = torch.Tensor([0])\n",
    "    for t in range(1,T-1):\n",
    "        s += (y[t]-phi*y[t-1])**2\n",
    "    return(-(T/2)*torch.log(torch.Tensor([2*math.pi]))+(1/2)*torch.log(1-phi**2) - (1/2)*(1-phi**2)*y[0]**2 - (1/2)*s)\n",
    "\n",
    "def log_likelihood(phi):\n",
    "    try:\n",
    "        ll = torch.Tensor(list(map(log_likelihood_float, phi)))\n",
    "    except:\n",
    "        ll = log_likelihood_float(phi)\n",
    "    return(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial guess of phi\n",
    "phi = torch.Tensor([0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.7347])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood(phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute values of $\\ell(\\phi)$ for all $-1 < \\phi < 1$ with increments of 0.1 and plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -0.9\n",
    "b = 0.9\n",
    "grid = torch.arange(a, b,0.1)\n",
    "ll_grid = log_likelihood(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the data\n",
    "plt.scatter(grid, ll_grid);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to find the 2 arg maximum values for phi and conducting another more granular grid search between the newly found values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = ll_grid.topk(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-5.7047, -5.7071]), tensor([ 11,  10]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new grid\n",
    "indices = indices.sort()[0]\n",
    "a = grid[indices[0]]\n",
    "b = grid[indices[1]]\n",
    "grid = torch.arange(a, b,0.01)\n",
    "ll_grid = log_likelihood(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the data\n",
    "plt.scatter(grid, ll_grid);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we stop here, we can find the estimated argmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle = grid[ll_grid.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1600)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can expand the algorithm to go through an arbitrary number of iteratons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(function, a=-1, b=1, n_grid = 10, n_iterations = 5):\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Divide the search interval into n_grid segments\n",
    "        step = (b-a)/n_grid\n",
    "        grid = torch.arange(a, b, step)\n",
    "        ll_grid = function(grid)\n",
    "\n",
    "        # Define solution\n",
    "        if i == n_iterations-1:\n",
    "            mle = grid[ll_grid.argmax()]\n",
    "            max_ll = ll_grid.max()\n",
    "            return (mle, max_ll)\n",
    "        # Define new search grif\n",
    "        else:\n",
    "            values, indices = ll_grid.topk(2)\n",
    "            indices = indices.sort()[0]\n",
    "            a = grid[indices[0]]\n",
    "            b = grid[indices[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1484), tensor(-5.7028))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search(log_likelihood, n_grid=3, n_iterations=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, as it is more commonly done, we can expand the algorithm to iterate enough times untile the required accuracy is desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(function, a=-1, b=1, n_grid = 10, accuracy = 0.01):\n",
    "    \n",
    "    i = 0\n",
    "    step = (b-a)/n_grid\n",
    "    \n",
    "    while(step>accuracy):\n",
    "        grid = torch.arange(a, b, step)\n",
    "        ll_grid = function(grid)\n",
    "        values, indices = ll_grid.topk(2)\n",
    "        indices = indices.sort()[0]\n",
    "        a = grid[indices[0]]\n",
    "        b = grid[indices[1]]\n",
    "        step = (b-a)/n_grid\n",
    "            \n",
    "    # Define solution\n",
    "    mle = grid[ll_grid.argmax()]\n",
    "    max_ll = ll_grid.max()\n",
    "    return (mle, max_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1485), tensor(-5.7028))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search(log_likelihood, n_grid=3, accuracy = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search can be a very good method when we want to optimize w.r.t a single parameter. When the dimension of $\\theta $ becomes large, it becomes quickly intractable. A method much more suited in this case is the steepest ascent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steepest Ascent (Gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm takes a differentiable functions and recursively updates its initial guess by moving in the direction of steepest ascent (indicated by the gradient). The speed of ascent is regulated by the learning rate hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analitically, we know that the argmax of the next function is (0,0)\n",
    "\n",
    "def log_likelihood(theta):\n",
    "    return(-1.5*theta[0]**2 - 2*theta[1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our function $\\ell(\\underline{\\theta})$, we need to find its gradient, $g(\\underline{\\theta})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(theta):\n",
    "    return(torch.tensor([-3*theta[0],- 4*theta[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initial guess for theta and learning rate parameter\n",
    "\n",
    "theta_0 = torch.Tensor([1,1])\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we establish the initial guess for the parameter: $\\underline{\\theta}^0$, the steepest ascent is a recursive algorithm for which at every step t (t>0), we have:\n",
    "\n",
    "$\\underline{\\theta}^{t} = \\underline{\\theta}^{t-1} + lr*g(\\underline{\\theta}^{t-1})$\n",
    "\n",
    "Which componentwise means:\n",
    "\n",
    "$\\theta_{it} = \\theta_{it-1} + lr*\\frac{\\partial\\ell}{\\partial\\theta_{i}}(\\theta_{it-1})$ \n",
    "\n",
    "And holds for each i. The algorithm can be iterated until some congergence criterion has been reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_ascent(function, gradient, guess = torch.Tensor([1,1]), lr=0.01, accuracy=0.0001):\n",
    "    \n",
    "    # Here, accuracy is expressed in terms of max component of the gradient\n",
    "    step = torch.ones(1)\n",
    "    while step.abs() > accuracy:\n",
    "        guess += lr*gradient(guess)\n",
    "        step = torch.max(gradient(guess))\n",
    "        \n",
    "    return (guess, function(guess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle, maximum = steepest_ascent(log_likelihood, gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-04 *\n",
      "       [ 3.6362,  0.2458])\n"
     ]
    }
   ],
   "source": [
    "print(mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton-Raphson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provided that second derivatives of the $\\ell$ function exist, and that $\\ell$ is concave, then the NR algorithm converges more quickly (with less iterations) compared to the steepest ascent algorithm. The algorithm is based on maximizing the 2 order taylor expansion of $\\ell$ aroung a guess which gets sequentially updated.\n",
    "Contrary to the previous example, we will use the differentiations abilities of pytorch in order to compute the second derivatives (hessian matrix) so that we do not have to derive them analytically and hardcode them. The recursive algorithm can be summarized in one equation:\n",
    "\n",
    "$\\underline{\\theta}^{t} = \\underline{\\theta}^{t-1} + lr*[-H(\\underline{\\theta}^{t-1})]^{-1}g(\\underline{\\theta}^{t-1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(theta):\n",
    "    res = -1.5*theta[0]**2 - 2*theta[1]**2\n",
    "    return(torch.stack([res]))\n",
    "\n",
    "def gradient(theta):\n",
    "    return(torch.tensor([-3*theta[0],- 4*theta[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = torch.Tensor([1,1])\n",
    "guess.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradient and Hessian using pytorch.autograd, code by apaszke\n",
    "# We need the code to compute a Jacobian because the Hessian is computed as the gradient's Jacobian\n",
    "\n",
    "def jacobian(fx, x, create_graph=False):                                                               \n",
    "    jac = []\n",
    "    # Reshapes the outputs of the function\n",
    "    flat_fx = fx.reshape(-1)\n",
    "    # Initializes grad_fx be the same size as the function output (if function outpus is a scalar, so grad_fx)\n",
    "    grad_fx = torch.zeros_like(flat_fx) \n",
    "    # loops over the function outputs (only one is f is a scalar valued function)\n",
    "    for i in range(len(flat_fx)):     \n",
    "        # Since we only want to compute the gradient wrt the ith output, we will one-hot encode it and feed it to torch.autograd\n",
    "        grad_fx[i] = 1. \n",
    "        # Computes the gradient of fx wrt x. grad_fx selects the component of f for which we want to compute the gradient\n",
    "        grad_x, = torch.autograd.grad(flat_fx, x, grad_fx, retain_graph=True, create_graph=create_graph)\n",
    "        # Appends the gradient wrt to the first output to the Jacobian (Jacobian = gradient for scalar valued functions)\n",
    "        jac.append(grad_x.reshape(x.shape))                                                           \n",
    "        grad_fx[i] = 0.\n",
    "    # Returns a tensor by stacking all the gradients and gives the tensor the dimension of n_output x n_inputs\n",
    "    return torch.stack(jac).reshape(fx.shape + x.shape)                                               \n",
    "                                                                                                      \n",
    "def hessian(y, x):       \n",
    "    H = jacobian(jacobian(y, x, create_graph=True), x)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3., -4.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that jacobian computes indeed the gradient (it should be [-3, -4])\n",
    "jacobian(log_likelihood(guess), guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the updating\n",
    "def NR_step(function, theta_0, lr = 0.1, gradient_output = False):\n",
    "    gradient = jacobian(log_likelihood(theta_0), theta_0)\n",
    "    gradient = gradient.reshape(2,1)\n",
    "    H = hessian(log_likelihood(theta_0), theta_0)\n",
    "    H = H.reshape(2,2)\n",
    "    inv_H = H.inverse()\n",
    "    theta_1 = theta_0 + lr * torch.mm(-inv_H, gradient).reshape(-1)\n",
    "    if gradient_output == False:\n",
    "        return (theta_1)\n",
    "    else:\n",
    "        return (theta_1, gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a recursion (th code would greatly benefit from a decaying structure of the lr, we avoid this step for the notebook purposes)\n",
    "\n",
    "grad_max = torch.ones(1)\n",
    "lr = 0.01\n",
    "max_steps = 1000\n",
    "accuracy = 0.01 # measured on the gradient\n",
    "guess_0 = torch.Tensor([1,1])\n",
    "guess_0.requires_grad = True\n",
    "step = 0\n",
    "\n",
    "while grad_max.numpy() > accuracy and step <= 1000:\n",
    "    guess_1, gradient = NR_step(log_likelihood, guess_0, lr = lr, gradient_output=True)\n",
    "    grad_max = torch.max(gradient).abs()\n",
    "    guess_0 = guess_1\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps:  568 \n",
      "Learning rate:  0.01 \n",
      "Argmax:  [0.00328417 0.00328417] \n",
      "Max:  [-3.7750156e-05]\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "print('Steps: ', step-1, \n",
    "      '\\nLearning rate: ', lr,\n",
    "     '\\nArgmax: ', guess_0.detach().numpy(),\n",
    "     '\\nMax: ', log_likelihood(guess_0).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\underline{\\theta}$ is n-dimensional, H will contain $\\frac{n(n-1)}{2}$ values to be computed. This can be inefficient as n gets large. An alternative could be to guess H at each step based on the gradients and arguments differences. This is based off the fact that the Hessian is simply the Jacobian of the gradient. These methods are known as modified Newton-Raphson."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
